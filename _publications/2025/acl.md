---
title:          "DiffPO: Diffusion-styled Preference Optimization for Efficient Inference-Time Alignment of Large Language Models"
date:           2025-2-01 00:01:00 +0800
selected:       true
pub:            "ACL"
pub_date:       "2025"
abstract: >-
  Diffusion-styled Preference Optimization (DPO) is a plug-and-play, policy-agnostic inference-time alignment method that aligns LLMs at the sentence level to reduce latency while improving alignment quality across benchmarks and model scales.
authors: []
links:
  Paper: https://arxiv.org/pdf/2503.04240?
---
